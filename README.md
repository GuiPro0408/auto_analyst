<p align="center">
  <img src="https://img.shields.io/badge/python-3.11+-blue.svg" alt="Python 3.11+">
  <img src="https://img.shields.io/badge/LangGraph-RAG%20Pipeline-green.svg" alt="LangGraph">
  <img src="https://img.shields.io/badge/Gemini-2.0%20Flash-orange.svg" alt="Gemini">
  <img src="https://img.shields.io/badge/license-MIT-lightgrey.svg" alt="License">
</p>

# ğŸ”¬ Auto-Analyst

> **An autonomous research assistant powered by a LangGraph RAG pipeline.**
>
> Plans queries, searches the web, chunks and embeds content, retrieves context, generates cited answers, and verifies claimsâ€”all using free/open-source components.

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ§  Intelligence
- **Multi-turn Conversation Memory** â€” Maintains context across follow-up questions
- **Query Classification** â€” Routes queries (factual/recommendation/creative) to appropriate prompts
- **Adaptive Research** â€” Iteratively refines search when results are insufficient
- **Quality Control** â€” Automatic assessment and improvement of answers

</td>
<td width="50%">

### ğŸ” Retrieval
- **Hybrid Search** â€” BM25 lexical + semantic embeddings with Reciprocal Rank Fusion
- **Contextual Chunking** â€” LLM-generated context per chunk (Anthropic's approach)
- **Cross-encoder Reranking** â€” Optional reranking for improved quality
- **Gemini Grounding Fast Path** â€” Direct answers from web-grounded responses

</td>
</tr>
<tr>
<td width="50%">

### ğŸ–¥ï¸ User Experience
- **Streaming Responses** â€” Real-time answer generation with Chainlit UI
- **Chat Persistence** â€” SQLite-backed conversation history
- **Multiple LLM Backends** â€” Gemini, Groq, HuggingFace, OpenAI-compatible

</td>
<td width="50%">

### âš™ï¸ Operations
- **API Key Rotation** â€” Automatic rotation on rate limits
- **Query Result Caching** â€” SQLite cache with TTL expiration
- **Comprehensive Logging** â€” Structured logs with run correlation IDs

</td>
</tr>
</table>

---

## ğŸš€ Quick Start

```bash
# Clone and setup
git clone https://github.com/<your-username>/auto-analyst.git && cd auto-analyst
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu

# Configure API keys (create .env file)
echo "GOOGLE_API_KEY=your_key_here" >> .env

# Run (choose one)
streamlit run ui/app.py              # Streamlit UI (http://localhost:8501)
chainlit run ui/chainlit_app.py -w   # Chainlit UI with streaming (http://localhost:8000)
```

---

## ğŸ”„ Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              USER QUERY                                     â”‚
â”‚                     "What are the effects of X?"                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  QUERY CLASSIFY   â”‚
                          â”‚  factual/recom-   â”‚
                          â”‚  mendation/       â”‚
                          â”‚  creative         â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PLAN                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Heuristic planner decomposes query into SearchQuery tasks           â”‚   â”‚
â”‚  â”‚ Detects time-sensitivity, topic, and conversation context           â”‚   â”‚
â”‚  â”‚ Example: ["effects of X on Y", "X statistics 2024"]                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  tools/planner.py â†’ List[SearchQuery]                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEARCH                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚    Tavily    â”‚  â”‚ Gemini Ground  â”‚  â”‚   Smart     â”‚                     â”‚
â”‚  â”‚   (API)      â”‚  â”‚ (Google search)â”‚  â”‚   Search    â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚  tools/search.py + tools/smart_search.py â†’ List[SearchResult]               â”‚
â”‚                                                                             â”‚
â”‚  Features: Domain filtering, deduplication, LLM result validation           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FETCH                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ robots.txt check â†’ Parallel download HTML/PDF â†’ Parse content       â”‚   â”‚
â”‚  â”‚ Configurable concurrency, retries, and backoff                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  tools/fetcher.py + tools/parser.py â†’ List[Document]                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHUNK                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Token-aware splitting (tiktoken) with configurable overlap          â”‚   â”‚
â”‚  â”‚ Optional contextual chunking: LLM adds document context per chunk   â”‚   â”‚
â”‚  â”‚ Metadata preserved: url, title, media_type, chunk_index             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  tools/chunker.py + tools/contextual_chunker.py â†’ List[Chunk]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EMBED & STORE                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ sentence-transformers (BAAI/bge-small-en-v1.5)                      â”‚   â”‚
â”‚  â”‚         â†“                                                            â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚
â”‚  â”‚ â”‚  ChromaDB   â”‚  â”‚    FAISS    â”‚  â”‚   Hybrid    â”‚                   â”‚   â”‚
â”‚  â”‚ â”‚ (persistent)â”‚  â”‚ (in-memory) â”‚  â”‚ (BM25+Emb)  â”‚                   â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  vector_store/*.py â†’ VectorStore.upsert(chunks)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RETRIEVE + RERANK                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Hybrid search: BM25 + semantic with Reciprocal Rank Fusion          â”‚   â”‚
â”‚  â”‚ Cosine similarity search â†’ Top-K chunks (default K=12)              â”‚   â”‚
â”‚  â”‚ Optional cross-encoder reranking (ms-marco-MiniLM-L-6-v2)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  tools/retriever.py + tools/reranker.py â†’ List[ScoredChunk]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚    ADAPTIVE       â”‚
                          â”‚  Assess context   â”‚
                          â”‚  relevance, may   â”‚
                          â”‚  trigger re-searchâ”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GENERATE                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Gemini 2.0 Flash (default) + Context â†’ Answer with [n] citations    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚ Query-type-specific prompts:                                         â”‚   â”‚
â”‚  â”‚ â€¢ Factual: Strict RAG with mandatory citations                       â”‚   â”‚
â”‚  â”‚ â€¢ Recommendation: LLM knowledge + RAG for suggestions                â”‚   â”‚
â”‚  â”‚ â€¢ Creative: Flexible LLM response with optional citations            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚ Fast path: Use Gemini grounded answer directly when available        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  tools/generator.py:generate_answer() â†’ (answer, citations)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERIFY                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ LLM reviews draft â†’ Removes unsupported claims â†’ Final answer       â”‚   â”‚
â”‚  â”‚ Preserves structure, formatting, and level of detail                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  tools/generator.py:verify_answer() â†’ verified_answer                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  QUALITY CONTROL  â”‚
                          â”‚  Assess answer    â”‚
                          â”‚  quality, may     â”‚
                          â”‚  trigger re-gen   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RESEARCH STATE                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ {                                                                    â”‚   â”‚
â”‚  â”‚   query, query_type, plan, search_results, documents, chunks,       â”‚   â”‚
â”‚  â”‚   retrieved, retrieval_scores, draft_answer, verified_answer,       â”‚   â”‚
â”‚  â”‚   citations, errors, warnings, adaptive_iterations, qc_passes,      â”‚   â”‚
â”‚  â”‚   conversation_history, grounded_answer, grounded_sources           â”‚   â”‚
â”‚  â”‚ }                                                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  api/state.py:ResearchState                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            STREAMLIT UI                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Answer with inline [1][2] citations + expandable source list        â”‚   â”‚
â”‚  â”‚ Conversation memory, API key status, debug panel                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  ui/app.py (Streamlit) or ui/chainlit_app.py (Chainlit with streaming)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<details>
<summary><strong>ğŸ“‹ Stage Summary</strong> (click to expand)</summary>

| Stage | Description |
|:------|:------------|
| ğŸ·ï¸ **Classify** | Routes query to factual/recommendation/creative mode |
| ğŸ“ **Plan** | Decomposes question into targeted search tasks |
| ğŸ” **Search** | Queries Tavily and/or Gemini Grounding with smart filtering |
| ğŸ“¥ **Fetch** | Downloads pages/PDFs in parallel, respecting robots.txt |
| âœ‚ï¸ **Chunk** | Token-aware splitting with optional contextual enrichment |
| ğŸ¯ **Retrieve** | Hybrid search (BM25+semantic) with optional cross-encoder rerank |
| ğŸ”„ **Adaptive** | Assesses context quality, triggers re-search if needed |
| âœï¸ **Generate** | LLM produces answer with `[n]` citations (query-type-aware) |
| âœ… **Verify** | Prunes unsupported claims while preserving structure |
| ğŸ† **QC** | Quality assessment and iterative improvement |

</details>

---

## âš™ï¸ Configuration

### ğŸ”‘ Core Settings

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_LLM` | `gemini-2.0-flash` | LLM model identifier |
| `AUTO_ANALYST_LLM_BACKEND` | `gemini` | LLM backend (`gemini`/`groq`/`huggingface`) |
| `AUTO_ANALYST_EMBED` | `BAAI/bge-small-en-v1.5` | Embedding model |
| `AUTO_ANALYST_VECTOR_STORE` | `chroma` | Vector store (`chroma`/`faiss`) |
| `AUTO_ANALYST_TOP_K` | `12` | Retrieved chunks per query |

> **ğŸ“ Note:** The default embedding model was changed from `all-MiniLM-L6-v2` to `BAAI/bge-small-en-v1.5` for improved retrieval quality. ChromaDB automatically detects and rebuilds incompatible vector stores.

<details>
<summary><strong>ğŸ” Search Settings</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_SEARCH_BACKENDS` | `tavily,gemini_grounding` | Comma-separated search backends |
| `AUTO_ANALYST_SMART_SEARCH` | `true` | LLM-assisted query analysis |
| `AUTO_ANALYST_VALIDATE_RESULTS` | `true` | LLM filtering of irrelevant hits |
| `AUTO_ANALYST_SEARCH_FALLBACK` | `true` | Fallback on rate limits |

</details>

<details>
<summary><strong>ğŸ”„ Pipeline Settings</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_ADAPTIVE_MAX_ITERS` | `2` | Max adaptive search cycles |
| `AUTO_ANALYST_QC_MAX_PASSES` | `1` | Max quality control passes |
| `AUTO_ANALYST_CHUNK_SIZE` | `1000` | Chunk size in tokens |
| `AUTO_ANALYST_CHUNK_OVERLAP` | `200` | Chunk overlap in tokens |
| `AUTO_ANALYST_ENABLE_RERANK` | `true` | Enable cross-encoder reranking |
| `AUTO_ANALYST_RERANK_MODEL` | `cross-encoder/ms-marco-MiniLM-L-6-v2` | Reranker model |

</details>

<details>
<summary><strong>ğŸ”€ Hybrid Search Settings</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_HYBRID_SEARCH` | `true` | Enable hybrid BM25 + semantic search |
| `AUTO_ANALYST_BM25_WEIGHT` | `0.3` | BM25 weight in rank fusion (0.0-1.0) |

</details>

<details>
<summary><strong>ğŸ“„ Contextual Chunking Settings</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_CONTEXTUAL_CHUNKS` | `true` | Enable LLM-generated chunk context |
| `AUTO_ANALYST_CONTEXTUAL_MAX_CHUNKS_PER_DOC` | `4` | Max chunks to contextualize per document |
| `AUTO_ANALYST_CONTEXTUAL_DOCUMENT_CHAR_LIMIT` | `8000` | Max document chars for context prompt |
| `AUTO_ANALYST_CONTEXTUAL_CHUNK_CHAR_LIMIT` | `1200` | Max chunk chars for context generation |

</details>

<details>
<summary><strong>ğŸ“¥ Fetcher Settings</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_FETCH_RETRIES` | `2` | Retry attempts per URL |
| `AUTO_ANALYST_FETCH_BACKOFF` | `1.0` | Backoff factor (seconds) |
| `AUTO_ANALYST_FETCH_CONCURRENCY` | `5` | Parallel fetch workers |
| `AUTO_ANALYST_FETCH_TIMEOUT` | `15` | Fetch timeout (seconds) |
| `AUTO_ANALYST_MIN_CONTENT_LENGTH` | `200` | Min chars for valid document |

</details>

<details>
<summary><strong>ğŸ’¾ Cache Settings</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_CACHE_PATH` | `data/query_cache.sqlite3` | Cache database path |
| `AUTO_ANALYST_CACHE_TTL` | `7200` | Cache TTL (seconds) |
| `AUTO_ANALYST_CACHE_MAX_ENTRIES` | `1000` | Max cached entries |

</details>

<details>
<summary><strong>ğŸ’¬ Conversation Memory</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_MEMORY_TURNS` | `5` | Conversation turns to remember |
| `AUTO_ANALYST_MEMORY_SUMMARY_CHARS` | `1200` | Max chars in history summary |
| `AUTO_ANALYST_ANSWER_PREVIEW_MAX_LEN` | `280` | Answer preview length in memory |

</details>

<details>
<summary><strong>ğŸ“Š Logging Settings</strong></summary>

| Variable | Default | Purpose |
|:---------|:--------|:--------|
| `AUTO_ANALYST_LOG_LEVEL` | `DEBUG` | Log level |
| `AUTO_ANALYST_LOG_FORMAT` | `plain` | Log format (`plain`/`json`) |
| `AUTO_ANALYST_LOG_FILE` | `auto_analyst.log` | Log file path |
| `AUTO_ANALYST_LOG_REDACT_QUERIES` | `false` | Redact queries in logs |

</details>

### ğŸ” API Keys & Secrets

> âš ï¸ **API credentials must be supplied through environment variables** (never hard-code them)

| Variable | Required For |
|:---------|:-------------|
| `GOOGLE_API_KEY` | Gemini LLM and grounding (single key) |
| `GOOGLE_API_KEYS` | Multiple Gemini keys for rotation |
| `GROQ_API_KEY` | Groq LLM backend |
| `GROQ_MODEL` | Groq model (default: `llama-3.3-70b-versatile`) |
| `HUGGINGFACE_API_TOKEN` | HuggingFace Inference backend |
| `TAVILY_API_KEY` | Tavily search backend |

<details>
<summary><strong>ğŸ“‹ Example .env file</strong></summary>

```bash
# Required: At least one Gemini API key
GOOGLE_API_KEY=your_gemini_key

# Optional: Multiple keys for rate limit rotation
GOOGLE_API_KEYS=key1,key2,key3

# Optional: Groq backend (fast inference)
GROQ_API_KEY=gsk_xxxxxxxxxxxxx
AUTO_ANALYST_LLM_BACKEND=groq

# Optional: Alternative backends
HUGGINGFACE_API_TOKEN=hf_xxxxxxxxxxxxx
TAVILY_API_KEY=tvly-xxxxxxxxxxxxx

# Recommended settings
AUTO_ANALYST_LLM_BACKEND=gemini
AUTO_ANALYST_SMART_SEARCH=true
AUTO_ANALYST_ENABLE_RERANK=true
AUTO_ANALYST_HYBRID_SEARCH=true
AUTO_ANALYST_CONTEXTUAL_CHUNKS=true
```

</details>

> ğŸ”’ **Security Note:** Keep `.env` out of version control and rotate any exposed keys.

---

## ğŸ› ï¸ Commands

```bash
# Setup
source .venv/bin/activate          # Activate virtualenv

# Run
streamlit run ui/app.py            # Streamlit UI (http://localhost:8501)
chainlit run ui/chainlit_app.py -w # Chainlit UI with streaming (http://localhost:8000)

# Testing
pytest                             # Run all tests
pytest -v                          # Verbose output
pytest -k "planner"                # Filter by name
pytest --cov=api --cov=tools       # With coverage

# Evaluation
python evaluation/run_evaluation.py --dataset data/sample_eval.json --model BAAI/bge-small-en-v1.5
```

---

## ğŸ“ Project Structure

```
ğŸ“¦ auto-analyst
â”œâ”€â”€ ğŸ”§ api/                 â†’ Orchestration, state management, caching
â”‚   â”œâ”€â”€ graph.py            # LangGraph pipeline nodes and edges (incl. streaming)
â”‚   â”œâ”€â”€ state.py            # Dataclasses and TypedDict definitions
â”‚   â”œâ”€â”€ state_builder.py    # State construction helpers
â”‚   â”œâ”€â”€ config.py           # Central configuration
â”‚   â”œâ”€â”€ logging_setup.py    # Structured logging with run correlation
â”‚   â”œâ”€â”€ cache_manager.py    # Query result caching
â”‚   â”œâ”€â”€ cache.py            # Cache encoding/decoding
â”‚   â”œâ”€â”€ key_rotator.py      # API key rotation for rate limits
â”‚   â””â”€â”€ memory.py           # Conversation history management
â”‚
â”œâ”€â”€ ğŸ› ï¸ tools/               â†’ Functional pipeline components
â”‚   â”œâ”€â”€ planner.py          # Query decomposition into search tasks
â”‚   â”œâ”€â”€ search.py           # Multi-backend web search
â”‚   â”œâ”€â”€ search_backends.py  # Backend implementations (Gemini, Tavily)
â”‚   â”œâ”€â”€ search_filters.py   # Result filtering and deduplication
â”‚   â”œâ”€â”€ smart_search.py     # LLM-powered search pipeline
â”‚   â”œâ”€â”€ fetcher.py          # URL fetching with robots.txt compliance
â”‚   â”œâ”€â”€ parser.py           # HTML/PDF content extraction
â”‚   â”œâ”€â”€ chunker.py          # Token-aware text splitting
â”‚   â”œâ”€â”€ contextual_chunker.py  # LLM-generated chunk context
â”‚   â”œâ”€â”€ generator.py        # LLM answer generation with citations
â”‚   â”œâ”€â”€ models.py           # LLM and embedding model loading
â”‚   â”œâ”€â”€ openai_compatible_llm.py  # OpenAI-compatible API wrapper (Groq, etc.)
â”‚   â”œâ”€â”€ reranker.py         # Cross-encoder reranking
â”‚   â”œâ”€â”€ retriever.py        # Vector similarity search
â”‚   â”œâ”€â”€ gemini_grounding.py # Gemini web-grounded responses
â”‚   â”œâ”€â”€ query_classifier.py # Query type classification
â”‚   â”œâ”€â”€ quality_control.py  # Answer quality assessment
â”‚   â”œâ”€â”€ adaptive_research.py # Context assessment and plan refinement
â”‚   â”œâ”€â”€ text_utils.py       # Shared text utilities
â”‚   â””â”€â”€ topic_utils.py      # Topic detection
â”‚
â”œâ”€â”€ ğŸ—„ï¸ vector_store/        â†’ Storage abstractions
â”‚   â”œâ”€â”€ base.py             # VectorStore abstract interface
â”‚   â”œâ”€â”€ chroma_store.py     # ChromaDB implementation (persistent)
â”‚   â”œâ”€â”€ faiss_store.py      # FAISS implementation (in-memory)
â”‚   â”œâ”€â”€ bm25_store.py       # BM25 lexical search store
â”‚   â””â”€â”€ hybrid_store.py     # Hybrid BM25+semantic with RRF fusion
â”‚
â”œâ”€â”€ ğŸ–¥ï¸ ui/                  â†’ User interfaces
â”‚   â”œâ”€â”€ app.py              # Streamlit application
â”‚   â”œâ”€â”€ chainlit_app.py     # Chainlit app with streaming support
â”‚   â””â”€â”€ data_layer.py       # SQLite-backed chat persistence
â”‚
â”œâ”€â”€ ğŸ“Š evaluation/          â†’ RAG evaluation metrics
â”‚   â”œâ”€â”€ metrics.py          # Context relevance, answer correctness, hallucination
â”‚   â””â”€â”€ run_evaluation.py   # Evaluation runner
â”‚
â””â”€â”€ ğŸ§ª tests/               â†’ pytest test suite
```

---

## ğŸ“ˆ Evaluation Metrics

The evaluation module (`evaluation/metrics.py`) provides embedding-based RAG metrics:

| Metric | Range | Interpretation |
|:-------|:-----:|:---------------|
| ğŸ“Š **Context Relevance** | 0-1 | Avg similarity between query and retrieved contexts |
| ğŸ“š **Context Sufficiency** | 0-1 | Fraction of contexts above relevance threshold |
| ğŸ¯ **Answer Relevance** | 0-1 | Similarity between generated and reference answers |
| âœ… **Answer Correctness** | 0-1 | Direct similarity to ground truth |
| âš ï¸ **Answer Hallucination** | 0-1 | Fraction of unsupported sentences (lower is better) |

---

## ğŸ—ï¸ Architecture Highlights

### ğŸ·ï¸ Query Classification

Queries are automatically classified to optimize answer generation:

| Type | Behavior | Example |
|:-----|:---------|:--------|
| **Factual** | Strict RAG with mandatory citations | News, research, technical |
| **Recommendation** | LLM knowledge enhanced by RAG context | Suggestions, opinions |
| **Creative** | Primarily LLM knowledge with optional citations | Brainstorming |

### ğŸ”„ Adaptive Research

When initial retrieval produces insufficient or low-relevance results:

```
assess_context() â†’ refine_plan() â†’ re-search â†’ fetch â†’ retrieve
        â†‘                                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    (max iterations configurable)
```

1. `assess_context()` evaluates chunk count and relevance scores
2. `refine_plan()` generates additional search tasks
3. Pipeline re-executes search â†’ fetch â†’ retrieve cycle
4. Maximum iterations configurable via `AUTO_ANALYST_ADAPTIVE_MAX_ITERS`

### âš¡ Gemini Grounding Fast Path

When Gemini's Google Search grounding returns a direct answer:

```
Query â†’ Gemini Grounding â†’ Direct Answer â†’ Citations from sources
              â†“
       (skips full RAG pipeline = faster)
```

### ğŸ”€ Hybrid Search

Combines BM25 lexical search with semantic embeddings using **Reciprocal Rank Fusion (RRF)**:

| Method | Excels At |
|:-------|:----------|
| **BM25** | Exact keyword matches (error codes, technical terms, names) |
| **Semantic** | Meaning and synonyms |

Configurable weighting via `AUTO_ANALYST_BM25_WEIGHT` (default: 0.3)

### ğŸ“„ Contextual Chunking

Based on [Anthropic's Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval):

1. ğŸ¤– LLM generates 2-3 sentence context for each chunk
2. ğŸ“ Context describes document topic, time period, and chunk's role
3. â• Context is prepended to chunk before embedding
4. ğŸ“ˆ Improves retrieval by preserving document-level information
5. ğŸ›¡ï¸ Circuit breaker prevents excessive LLM calls on rate limits

### ğŸ”‘ API Key Rotation

For high-volume usage with rate limits:

```
Request â†’ Key1 (429) â†’ Key2 (429) â†’ Key3 â†’ Success â†’ Reset all keys
```

1. Configure multiple keys: `GOOGLE_API_KEYS=key1,key2,key3`
2. `APIKeyRotator` automatically rotates on 429 errors
3. Keys are reset after successful requests
4. UI shows real-time key availability status

---

## ğŸ“‹ Prerequisites

| Requirement | Details |
|:------------|:--------|
| **Python** | 3.11+ |
| **Disk Space** | ~4GB for models (embeddings + optional reranker) |

---

## ğŸ–¥ï¸ UI Options

<table>
<tr>
<td width="50%">

### Streamlit (Basic)
```bash
streamlit run ui/app.py
```

- âœ… Simple chat interface
- ğŸ”§ Debug panel with pipeline details
- ğŸ”‘ API key status display

</td>
<td width="50%">

### Chainlit (Production)
```bash
chainlit run ui/chainlit_app.py -w
```

- âš¡ **Streaming responses** â€” Real-time answer generation
- ğŸ’¾ **Chat persistence** â€” SQLite-backed conversation history
- ğŸ“Š **Step visualization** â€” Shows pipeline progress
- ğŸ’¬ **Multi-turn support** â€” Automatic conversation context

</td>
</tr>
</table>

---

## ğŸ“ Notes

| | |
|:---|:---|
| ğŸ†“ | **No paid APIs required** â€” uses Gemini free tier, open-source models |
| ğŸ¤– | **robots.txt compliance** â€” fetcher respects site restrictions |
| ğŸ”„ | **Automatic model migration** â€” ChromaDB detects and rebuilds incompatible embeddings |
| ğŸ“š | See `ressources/*.md` for technical design and evaluation methodology |
| ğŸ” | Adaptive research: automatically broadens search when context is thin |
| âœ¨ | Quality control: optional refinement loop to improve answers |

---

<p align="center">
  <sub>Built with â¤ï¸ using LangGraph, Gemini, and open-source tools</sub>
</p>
